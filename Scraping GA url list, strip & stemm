# Imports
import bs4 as bs
import urllib.request
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re

from nltk.corpus import stopwords
from nltk.cluster.util import cosine_distance
import numpy as np
import networkx as nx

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

# Scrape demo content using url list without the company name in it!
df_ga = pd.read_csv('urls_recommender_nov.csv', sep = ';')
df_ga['Page'] = df_ga['Page'].str.lower()
search ="/items/nl-nl/vacatures/"
bool_series = df_ga["Page"].str.startswith(search, na = False) 
df_ga = df_ga[bool_series]
df_ga['Cat'] = df_ga['Page'].str.split('/').str[4]
vakgebieden = ['data-en-analytics' , 'it', 'ict', 'traineeship']
booly = df_ga['Cat'].isin(vakgebieden)
df_ga = df_ga[booly]
df_ga['Url'] = "https://www.werkenbijanwb.nl"+ df_ga["Page"] 
list_url = df_ga['Url'].tolist()

ListNummer = 0
ListTotaal = []


for x in list_url:
    try:
        source = urllib.request.urlopen(x).read()
        soup = bs.BeautifulSoup(source,'lxml')
        title = soup.title.string
        ListNaam = "Vacature" + str(ListNummer)
        ListNummer = ListNummer + 1
        #print(title, "boven loop")
        for div in soup.find_all('span', class_ = 'extText'):
            Listy = []
            Listy.append(ListNaam)
            Listy.append(str(title))
            #print(title, "Onderloop")
            Listy.append(div.text)
            #print('Scraping succesfull')
    except:
        print(x, 'Scraping failed - URL not found.')
        
    ListTotaal.append(Listy)

# After scrape make DF
df_tekst = pd.DataFrame(ListTotaal, columns =['Nummer', 'Vacture', 'Tekst'])
df_tekst = df_tekst.drop_duplicates(subset = 'Nummer', keep = 'first')

# Strip all sentences and make them lowercase.
def sentence(x):
    tekst = x
    zinnen = tekst.replace("[^a-zA-Z]", " ").split(". ")
    #zinlijst.append(zinnen.lower().split(" "))
    return zinnen

df_tekst['Los'] = df_tekst['Tekst'].str.lower().str.replace('\W', ' ').apply(sentence)

## Stopwords imports

stopwords = nltk.corpus.stopwords.words('dutch')
stopwords.extend(['anwb', 'demo2', 'jij', 'heb', 'als'] )

#Stopword removal
df_tekst['Stopaf'] = df_tekst['Los'].apply(lambda x: ' '.join([word for word in x[0].split() if word not in (stopwords)]))

# However, for the next sectopn, it needs to be in a nested list, or single list per cell. Hence, we add the content of every
# cell to list with a quick function.
def tolistor(x):
    listy = []
    listy.append(x)
    return listy

df_tekst['Stopaf'] = df_tekst['Stopaf'].apply(tolistor)

# Now we can safely continue
# Stemming

from nltk.stem.snowball import SnowballStemmer

# Demo the technique
# stemmer.stem('medewerkers')

# Import Dutch stemmer.
stemmer = SnowballStemmer("dutch")

df_tekst['Stemmed'] = df_tekst['Stopaf'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.


# Extract text from list and insert it back into the DF with a function. 
def selector(x):
    return x[0]

df_tekst['Stemmed'] = df_tekst['Stemmed'].apply(selector)

# Display df
df_tekst.head()
